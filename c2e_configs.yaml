# c2e transformer project config fileï¼š
wandb_entity: ''  # write your own wandb_entity
# following params need to change before you start every trail
trial_id: 10007 # plus 1 when you're going to start
ckpt_file_name: 'c2e_transformer_[0526-test1].pt'
log_file_name: 'c2e_transformer_[0526-test1].log' # in this log you can track the config file content and the trial id.

ckpt_dir: './models'
log_dir: './logs'

# specify model
require_load_model: False
model_load_path: './c2e_transformer_[0526-test1].pt' # The structure and params should be saved as a whole
model_to_infer_path: null

n_head: 8
n_layer: 6

max_trg_sent_len: 128
refer_max_tensor_len: 32

pad_token: 0
bos_token: 1
eos_token: 2
unk_token: 3

beam_size: 5

model_dim: 512
hidden_dim: 2048

train_ratio: 0.98
val_ratio: 0.01


# training params
max_epochs: 30
batch_size: 50
learning_rate: 5.0e-4 # use float form or complete digital number
drop_prob: 0.15
clip_flag: False
clip_norm: null # use null to represent None in python
scheduler_flag: True
scheduler_type: 'cosine'
ignore_index: 0
anneal_rate: 0.8
patience: 10
threshold: 0.01
optimizer_type: 'adam'
loss_type: 'ce'
warmup: 5
smoothing: 0.01
# screening parameters
print_freq: 300 # training step (step means fit one batch)
eval_freq: 1 # training epoch
save_freq: 1 # training epoch

# typically you don't need to change the following params
device: torch.device("cuda" if torch.cuda.is_available() else "cpu") # use 'eval()' function
random_seed: 0
CUBLAS_WORKSPACE_CONFIG: ':4096:8'